{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "59ed05bb-fe72-45c8-aaf4-f067ee02838e",
      "metadata": {
        "id": "59ed05bb-fe72-45c8-aaf4-f067ee02838e"
      },
      "source": [
        "# Classification using Deep Learning with Histogram data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13095d08-82fa-49dc-8613-e8d4d1320555",
      "metadata": {
        "id": "13095d08-82fa-49dc-8613-e8d4d1320555"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5f39eb1-6f50-40c3-8468-226c0c8ee6b4",
      "metadata": {
        "id": "d5f39eb1-6f50-40c3-8468-226c0c8ee6b4"
      },
      "source": [
        "### Reading the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97f576c9-784f-49d2-bf5d-6978edeb89d4",
      "metadata": {
        "id": "97f576c9-784f-49d2-bf5d-6978edeb89d4"
      },
      "source": [
        "First, we’ll load the saved image and label data from the NumPy files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GjekVGlXEFHT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjekVGlXEFHT",
        "outputId": "048a44a4-a01e-4d99-b38a-f60b875a097b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Caminho base para os arquivos .npy\n",
        "base_path = '/content/drive/MyDrive/cubsat'\n",
        "\n",
        "# Carregamento dos arquivos com os nomes esperados\n",
        "train_images = np.load(f'{base_path}/train_images.npy')\n",
        "train_labels = np.load(f'{base_path}/train_labels.npy')\n",
        "val_images   = np.load(f'{base_path}/val_images.npy')\n",
        "val_labels   = np.load(f'{base_path}/val_labels.npy')\n",
        "\n",
        "# Exibir os shapes para conferência\n",
        "print(\"train_images shape:\", train_images.shape)\n",
        "print(\"train_labels shape:\", train_labels.shape)\n",
        "print(\"val_images shape:\", val_images.shape)\n",
        "print(\"val_labels shape:\", val_labels.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03a0d2f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03a0d2f2",
        "outputId": "9b79d46e-1dc7-4ab5-c111-8775ff1a9eff"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Display image information\n",
        "print(\"=== Image Characteristics ===\")\n",
        "print(f\"Shape of training set: {train_images.shape} (Samples, Height, Width, Channels)\")\n",
        "print(f\"Shape of validation set: {val_images.shape} (Samples, Height, Width, Channels)\")\n",
        "\n",
        "# Image resolution (height and width)\n",
        "image_resolution = train_images.shape[1:3]  # Taking height and width dimensions\n",
        "print(f\"Image resolution: {image_resolution} pixels\")\n",
        "\n",
        "# Total number of images\n",
        "print(f\"Number of images in training set: {train_images.shape[0]}\")\n",
        "print(f\"Number of images in validation set: {val_images.shape[0]}\")\n",
        "\n",
        "# Data type of the images\n",
        "print(f\"Data type of images: {train_images.dtype}\")\n",
        "\n",
        "# Range of values in the images\n",
        "print(f\"Minimum and maximum values in training dataset: {train_images.min()} to {train_images.max()}\")\n",
        "\n",
        "# Display some unique labels and their counts in the training set\n",
        "unique_labels_train, counts_train = np.unique(train_labels, return_counts=True)\n",
        "print(\"\\n=== Class Distribution in Training Set ===\")\n",
        "for label, count in zip(unique_labels_train, counts_train):\n",
        "    print(f\"Class {label}: {count} samples\")\n",
        "\n",
        "# Display some unique labels and their counts in the validation set\n",
        "unique_labels_val, counts_val = np.unique(val_labels, return_counts=True)\n",
        "print(\"\\n=== Class Distribution in Validation Set ===\")\n",
        "for label, count in zip(unique_labels_val, counts_val):\n",
        "    print(f\"Class {label}: {count} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad3d8f2e",
      "metadata": {
        "id": "ad3d8f2e"
      },
      "source": [
        "## Combining training and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iZphsFEdJE7E",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZphsFEdJE7E",
        "outputId": "024f8884-578c-4ca5-8e2b-3a20dc1c3be6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Novo diretório onde os arquivos combinados serão salvos\n",
        "cached_data_dir = '/content/drive/MyDrive/cubsat'\n",
        "os.makedirs(cached_data_dir, exist_ok=True)\n",
        "\n",
        "# Caminhos dos arquivos .npy combinados\n",
        "all_images_path = os.path.join(cached_data_dir, 'all_images.npy')\n",
        "all_labels_path = os.path.join(cached_data_dir, 'all_labels.npy')\n",
        "\n",
        "# Verifica se os arquivos já existem\n",
        "if os.path.exists(all_images_path) and os.path.exists(all_labels_path):\n",
        "    print(\"Combined files already exist. Loading from disk...\")\n",
        "    all_images = np.load(all_images_path)\n",
        "    all_labels = np.load(all_labels_path)\n",
        "else:\n",
        "    print(\"Combined files do not exist. Concatenating data...\")\n",
        "    # Concatena os conjuntos de treino e validação\n",
        "    all_images = np.concatenate((train_images, val_images), axis=0)\n",
        "    all_labels = np.concatenate((train_labels, val_labels), axis=0)\n",
        "\n",
        "    # Salva os arquivos combinados\n",
        "    np.save(all_images_path, all_images)\n",
        "    np.save(all_labels_path, all_labels)\n",
        "    print(\"Data concatenated and saved.\")\n",
        "\n",
        "# Exibe informações sobre as imagens\n",
        "print(\"=== Image Characteristics ===\")\n",
        "print(f\"Shape of combined dataset: {all_images.shape} (Samples, Height, Width, Channels)\")\n",
        "print(f\"Image resolution: {all_images.shape[1:3]} pixels\")\n",
        "print(f\"Total number of images: {all_images.shape[0]}\")\n",
        "print(f\"Data type of images: {all_images.dtype}\")\n",
        "print(f\"Minimum and maximum values in combined dataset: {all_images.min()} to {all_images.max()}\")\n",
        "\n",
        "# Exibe distribuição das classes\n",
        "unique_labels, counts = np.unique(all_labels, return_counts=True)\n",
        "print(\"\\n=== Class Distribution in Combined Dataset ===\")\n",
        "for label, count in zip(unique_labels, counts):\n",
        "    print(f\"Class {label}: {count} samples\")\n",
        "\n",
        "print(f\"\\nData saved in: {cached_data_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Gd2WuAMwJV0E",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gd2WuAMwJV0E",
        "outputId": "bac14b09-f6a5-4232-ed1a-7e146550503e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Caminho do arquivo\n",
        "all_images_path = '/content/drive/MyDrive/cubsat/all_images.npy'\n",
        "\n",
        "# Carregando o arquivo .npy\n",
        "all_images = np.load(all_images_path)\n",
        "\n",
        "# Verificando forma e tipo dos dados\n",
        "print(\"Shape:\", all_images.shape)\n",
        "print(\"Dtype:\", all_images.dtype)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ooy80GTtNI-r",
      "metadata": {
        "id": "Ooy80GTtNI-r"
      },
      "source": [
        "PLOTANDO AMOSTRAS ALEATÓRIAS DE CADA CLASSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YDOsYKeUKMl4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YDOsYKeUKMl4",
        "outputId": "60127ac2-43cf-4235-c8fb-868ecc79224c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import random\n",
        "\n",
        "# Caminhos dos arquivos\n",
        "all_images_path = '/content/drive/MyDrive/cubsat/all_images.npy'\n",
        "all_labels_path = '/content/drive/MyDrive/cubsat/all_labels.npy'\n",
        "\n",
        "# Carregando os dados\n",
        "all_images = np.load(all_images_path)\n",
        "all_labels = np.load(all_labels_path)\n",
        "\n",
        "# Identificando classes únicas\n",
        "unique_classes = np.unique(all_labels)\n",
        "\n",
        "# Plotando uma imagem aleatória de cada classe\n",
        "for cls in unique_classes:\n",
        "    # Índices das imagens da classe atual\n",
        "    indices = np.where(all_labels == cls)[0]\n",
        "    # Escolhendo um índice aleatório\n",
        "    random_index = random.choice(indices)\n",
        "    # Selecionando a imagem\n",
        "    image = all_images[random_index]\n",
        "\n",
        "    # Plotando com Plotly\n",
        "    fig = px.imshow(image.astype(np.uint8))\n",
        "    fig.update_layout(title=f\"Class {cls}\")\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a739cd9",
      "metadata": {
        "id": "3a739cd9"
      },
      "source": [
        "## APLICANDO DATA AUGMENTATION PARA BALANCEAMENTO DE CLASSES"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q6emBRccN1pN",
      "metadata": {
        "id": "q6emBRccN1pN"
      },
      "source": [
        " Mostrar uma imagem aleatória com sua classe (após BALANCEAMENTO):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jY5UwembTEP3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jY5UwembTEP3",
        "outputId": "48133c6f-70e0-49f5-b34f-95eb1a7207af"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Diretório dos arquivos originais\n",
        "base_dir = '/content/drive/MyDrive/cubsat'\n",
        "all_images_path = os.path.join(base_dir, 'all_images.npy')\n",
        "all_labels_path = os.path.join(base_dir, 'all_labels.npy')\n",
        "\n",
        "# Diretório para salvar dados balanceados\n",
        "ros_dir = os.path.join(base_dir, \"RandomOversampling\")\n",
        "os.makedirs(ros_dir, exist_ok=True)\n",
        "\n",
        "# Caminhos dos arquivos balanceados\n",
        "ros_images_path = os.path.join(ros_dir, \"all_images_ros.npy\")\n",
        "ros_labels_path = os.path.join(ros_dir, \"all_labels_ros.npy\")\n",
        "\n",
        "# Carregar dados originais\n",
        "all_images = np.load(all_images_path)\n",
        "all_labels = np.load(all_labels_path)\n",
        "\n",
        "# Verifica se os arquivos já existem\n",
        "if os.path.exists(ros_images_path) and os.path.exists(ros_labels_path):\n",
        "    print(\"Random Oversampled files already exist. Loading from disk...\")\n",
        "    X_resampled = np.load(ros_images_path)\n",
        "    y_resampled = np.load(ros_labels_path)\n",
        "else:\n",
        "    print(\"Applying Random Oversampling...\")\n",
        "\n",
        "    # Flatten temporariamente para aplicar o oversampling\n",
        "    num_samples, height, width, channels = all_images.shape\n",
        "    all_images_flat = all_images.reshape(num_samples, -1)\n",
        "\n",
        "    # Aplicar RandomOverSampler\n",
        "    ros = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
        "    X_resampled_flat, y_resampled = ros.fit_resample(all_images_flat, all_labels)\n",
        "\n",
        "    # Voltar ao formato original\n",
        "    X_resampled = X_resampled_flat.reshape(-1, height, width, channels)\n",
        "\n",
        "    # Embaralhar\n",
        "    X_resampled, y_resampled = shuffle(X_resampled, y_resampled, random_state=42)\n",
        "\n",
        "    # Salvar\n",
        "    np.save(ros_images_path, X_resampled)\n",
        "    np.save(ros_labels_path, y_resampled)\n",
        "    print(\"Random Oversampling applied and saved.\")\n",
        "\n",
        "# Mostrar distribuição das classes\n",
        "unique_labels, counts = np.unique(y_resampled, return_counts=True)\n",
        "print(\"\\n=== Class Distribution after Random Oversampling ===\")\n",
        "for label, count in zip(unique_labels, counts):\n",
        "    print(f\"Class {label}: {count} samples\")\n",
        "\n",
        "print(f\"\\nBalanced data saved in: {ros_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gWALjkMxTGp_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "gWALjkMxTGp_",
        "outputId": "c16d50d6-a8e3-4dc1-c02b-ef1a62675591"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import plotly.express as px\n",
        "import random\n",
        "\n",
        "# Caminhos\n",
        "ros_dir = '/content/drive/MyDrive/cubsat/RandomOversampling'\n",
        "ros_images_path = os.path.join(ros_dir, \"all_images_ros.npy\")\n",
        "ros_labels_path = os.path.join(ros_dir, \"all_labels_ros.npy\")\n",
        "\n",
        "# Carregar dados\n",
        "images = np.load(ros_images_path)\n",
        "labels = np.load(ros_labels_path)\n",
        "\n",
        "# Identificar classes únicas\n",
        "unique_classes = np.unique(labels)\n",
        "\n",
        "# Criar subplots em plotly\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "n_classes = len(unique_classes)\n",
        "fig = make_subplots(rows=1, cols=n_classes, subplot_titles=[f\"Class {cls}\" for cls in unique_classes])\n",
        "\n",
        "# Adicionar uma imagem aleatória de cada classe\n",
        "for idx, cls in enumerate(unique_classes):\n",
        "    indices = np.where(labels == cls)[0]\n",
        "    selected_idx = random.choice(indices)\n",
        "    img = images[selected_idx]\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Image(z=img.astype(np.uint8)),\n",
        "        row=1, col=idx+1\n",
        "    )\n",
        "\n",
        "# Layout\n",
        "fig.update_layout(height=300, width=200 * n_classes, title_text=\"Random Example from Each Class (Balanced Dataset)\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9849d30b",
      "metadata": {
        "id": "9849d30b"
      },
      "source": [
        "## Splitting combined set in proportion 80% for training and 20% for validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u5BFldJWhFpB",
      "metadata": {
        "id": "u5BFldJWhFpB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "\n",
        "# Caminhos\n",
        "ros_dir = '/content/drive/MyDrive/cubsat/RandomOversampling'\n",
        "ros_images_path = os.path.join(ros_dir, \"all_images_ros.npy\")\n",
        "ros_labels_path = os.path.join(ros_dir, \"all_labels_ros.npy\")\n",
        "\n",
        "# Carregar dados\n",
        "images = np.load(ros_images_path)\n",
        "labels = np.load(ros_labels_path)\n",
        "\n",
        "# Identificar classes únicas\n",
        "unique_classes = np.unique(labels)\n",
        "\n",
        "# Listas para armazenar os dados divididos\n",
        "train_images, train_labels = [], []\n",
        "val_images, val_labels = [], []\n",
        "\n",
        "# Garantir reprodução\n",
        "random.seed(42)\n",
        "\n",
        "# Dividir os dados mantendo o balanceamento\n",
        "for cls in unique_classes:\n",
        "    indices = np.where(labels == cls)[0]\n",
        "    random.shuffle(indices.tolist())\n",
        "\n",
        "    split_idx = int(0.8 * len(indices))\n",
        "    train_idx = indices[:split_idx]\n",
        "    val_idx = indices[split_idx:]\n",
        "\n",
        "    train_images.append(images[train_idx])\n",
        "    train_labels.append(labels[train_idx])\n",
        "    val_images.append(images[val_idx])\n",
        "    val_labels.append(labels[val_idx])\n",
        "\n",
        "# Concatenar resultados\n",
        "train_images = np.concatenate(train_images, axis=0)\n",
        "train_labels = np.concatenate(train_labels, axis=0)\n",
        "val_images = np.concatenate(val_images, axis=0)\n",
        "val_labels = np.concatenate(val_labels, axis=0)\n",
        "\n",
        "# Salvar os arquivos\n",
        "np.save(os.path.join(ros_dir, \"train_images_ros.npy\"), train_images)\n",
        "np.save(os.path.join(ros_dir, \"train_labels_ros.npy\"), train_labels)\n",
        "np.save(os.path.join(ros_dir, \"val_images_ros.npy\"), val_images)\n",
        "np.save(os.path.join(ros_dir, \"val_labels_ros.npy\"), val_labels)\n",
        "\n",
        "print(\"Conjuntos salvos com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3MWDUt77sn3h",
      "metadata": {
        "id": "3MWDUt77sn3h"
      },
      "source": [
        "GERANDO IMAGENS ALEATORIAS DE CADA CLASSE POR CONJUNTO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xToqOV1ShIBY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "xToqOV1ShIBY",
        "outputId": "a49b389a-ef8a-4b84-f54d-a97a0ae51ecd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Caminho\n",
        "ros_dir = '/content/drive/MyDrive/cubsat/RandomOversampling'\n",
        "train_images = np.load(os.path.join(ros_dir, \"train_images_ros.npy\"))\n",
        "train_labels = np.load(os.path.join(ros_dir, \"train_labels_ros.npy\"))\n",
        "\n",
        "# Classes únicas\n",
        "unique_classes = np.unique(train_labels)\n",
        "n_classes = len(unique_classes)\n",
        "\n",
        "# Criar subplots\n",
        "fig = make_subplots(rows=1, cols=n_classes, subplot_titles=[f\"Class {cls}\" for cls in unique_classes])\n",
        "\n",
        "# Adicionar imagem aleatória de cada classe\n",
        "for idx, cls in enumerate(unique_classes):\n",
        "    indices = np.where(train_labels == cls)[0]\n",
        "    selected_idx = random.choice(indices)\n",
        "    img = train_images[selected_idx]\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Image(z=img.astype(np.uint8)),\n",
        "        row=1, col=idx+1\n",
        "    )\n",
        "\n",
        "fig.update_layout(height=300, width=200 * n_classes, title_text=\"Treinamento: Uma imagem aleatória por classe\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GcPVuZVthH3-",
      "metadata": {
        "id": "GcPVuZVthH3-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Caminho\n",
        "ros_dir = '/content/drive/MyDrive/cubsat/RandomOversampling'\n",
        "val_images = np.load(os.path.join(ros_dir, \"val_images_ros.npy\"))\n",
        "val_labels = np.load(os.path.join(ros_dir, \"val_labels_ros.npy\"))\n",
        "\n",
        "# Classes únicas\n",
        "unique_classes = np.unique(val_labels)\n",
        "n_classes = len(unique_classes)\n",
        "\n",
        "# Criar subplots\n",
        "fig = make_subplots(rows=1, cols=n_classes, subplot_titles=[f\"Class {cls}\" for cls in unique_classes])\n",
        "\n",
        "# Adicionar imagem aleatória de cada classe\n",
        "for idx, cls in enumerate(unique_classes):\n",
        "    indices = np.where(val_labels == cls)[0]\n",
        "    selected_idx = random.choice(indices)\n",
        "    img = val_images[selected_idx]\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Image(z=img.astype(np.uint8)),\n",
        "        row=1, col=idx+1\n",
        "    )\n",
        "\n",
        "fig.update_layout(height=300, width=200 * n_classes, title_text=\"Validação: Uma imagem aleatória por classe\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r2jYzsxdqiW7",
      "metadata": {
        "id": "r2jYzsxdqiW7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import collections\n",
        "\n",
        "# Caminho\n",
        "ros_dir = '/content/drive/MyDrive/cubsat/RandomOversampling'\n",
        "\n",
        "# Carregar labels\n",
        "train_labels = np.load(os.path.join(ros_dir, \"train_labels_ros.npy\"))\n",
        "val_labels = np.load(os.path.join(ros_dir, \"val_labels_ros.npy\"))\n",
        "\n",
        "# Contar classes\n",
        "train_counts = collections.Counter(train_labels)\n",
        "val_counts = collections.Counter(val_labels)\n",
        "\n",
        "# Exibir resultados\n",
        "print(\"Distribuição das classes no conjunto de TREINAMENTO:\")\n",
        "for cls, count in sorted(train_counts.items()):\n",
        "    print(f\"Classe {cls}: {count} imagens\")\n",
        "\n",
        "print(\"\\nDistribuição das classes no conjunto de VALIDAÇÃO:\")\n",
        "for cls, count in sorted(val_counts.items()):\n",
        "    print(f\"Classe {cls}: {count} imagens\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UE21UV_Ht9hB",
      "metadata": {
        "id": "UE21UV_Ht9hB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Caminhos atualizados\n",
        "ros_dir = '/content/drive/MyDrive/cubsat/RandomOversampling'\n",
        "bin_edges_path = os.path.join(ros_dir, 'bin_edges_ros.npy')\n",
        "\n",
        "# Carregar dados de treino e validação\n",
        "train_images = np.load(os.path.join(ros_dir, 'train_images_ros.npy'))\n",
        "train_labels = np.load(os.path.join(ros_dir, 'train_labels_ros.npy'))\n",
        "val_images = np.load(os.path.join(ros_dir, 'val_images_ros.npy'))\n",
        "val_labels = np.load(os.path.join(ros_dir, 'val_labels_ros.npy'))\n",
        "\n",
        "# Verificar se o arquivo de bin_edges já existe\n",
        "if os.path.exists(bin_edges_path):\n",
        "    print(\"Arquivo de bin_edges já existe. Carregando do disco...\")\n",
        "    bin_edges = np.load(bin_edges_path, allow_pickle=True)\n",
        "else:\n",
        "    print(\"Arquivo de bin_edges não encontrado. Calculando quantis...\")\n",
        "\n",
        "    # Definir os níveis de quantis (decis -> 10 intervalos)\n",
        "    quantile_levels = np.linspace(0, 1, num=11)\n",
        "    bin_edges = []\n",
        "\n",
        "    # Calcular os quantis para cada canal (R, G, B)\n",
        "    for channel in range(3):\n",
        "        channel_pixels = train_images[:, :, :, channel].flatten()\n",
        "        edges = np.quantile(channel_pixels, quantile_levels)\n",
        "        bin_edges.append(edges)\n",
        "\n",
        "    # Salvar os bin_edges\n",
        "    np.save(bin_edges_path, bin_edges)\n",
        "    print(\"Quantis calculados e bin_edges salvos.\")\n",
        "\n",
        "# Mostrar os bin_edges\n",
        "print(\"\\n=== Bin Edges por Canal ===\")\n",
        "for channel, edges in enumerate(bin_edges):\n",
        "    print(f\"Canal {channel} (R={channel==0}, G={channel==1}, B={channel==2}): {edges}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BrDE20wpv-N_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrDE20wpv-N_",
        "outputId": "f3f8cfcf-b832-4f54-96aa-069bc48b199c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import gc\n",
        "\n",
        "# Caminho base\n",
        "ros_dir = '/content/drive/MyDrive/cubsat/RandomOversampling'\n",
        "\n",
        "# Caminhos dos arquivos\n",
        "train_images_path = os.path.join(ros_dir, 'train_images_ros.npy')\n",
        "train_labels_path = os.path.join(ros_dir, 'train_labels_ros.npy')\n",
        "val_images_path = os.path.join(ros_dir, 'val_images_ros.npy')\n",
        "val_labels_path = os.path.join(ros_dir, 'val_labels_ros.npy')\n",
        "bin_edges_path = os.path.join(ros_dir, 'bin_edges_ros.npy')\n",
        "\n",
        "# Carregar dados\n",
        "print(\"Carregando dados de treino e validação...\")\n",
        "train_images = np.load(train_images_path)\n",
        "train_labels = np.load(train_labels_path)\n",
        "val_images = np.load(val_images_path)\n",
        "val_labels = np.load(val_labels_path)\n",
        "\n",
        "# Verificar existência de bin_edges\n",
        "if os.path.exists(bin_edges_path):\n",
        "    print(\"Arquivo bin_edges_ros.npy já existe. Carregando...\")\n",
        "    bin_edges = np.load(bin_edges_path, allow_pickle=True)\n",
        "else:\n",
        "    print(\"Calculando bin_edges (quantis)...\")\n",
        "    quantile_levels = np.linspace(0, 1, num=11)  # decílicos\n",
        "    bin_edges = []\n",
        "    for channel in range(3):  # R, G, B\n",
        "        pixels = train_images[:, :, :, channel].flatten()\n",
        "        edges = np.quantile(pixels, quantile_levels)\n",
        "        bin_edges.append(edges)\n",
        "    np.save(bin_edges_path, bin_edges)\n",
        "    print(\"bin_edges salvos em disco.\")\n",
        "\n",
        "# Exibir bin_edges (opcional)\n",
        "print(\"\\n=== Bin Edges por Canal ===\")\n",
        "for ch, edges in enumerate(bin_edges):\n",
        "    print(f\"Canal {ch}: {edges}\")\n",
        "\n",
        "# Função para converter imagem em histograma normalizado\n",
        "def image_to_histogram(image, bin_edges):\n",
        "    features = []\n",
        "    for channel in range(3):  # R, G, B\n",
        "        pixels = image[:, :, channel].flatten()\n",
        "        hist, _ = np.histogram(pixels, bins=bin_edges[channel])\n",
        "        hist = hist / len(pixels)  # Normaliza para proporção\n",
        "        features.extend(hist)\n",
        "    return np.array(features)\n",
        "\n",
        "# Converter imagens de treino em histogramas\n",
        "print(\"\\nConvertendo imagens de treino...\")\n",
        "train_histograms = np.array([image_to_histogram(img, bin_edges) for img in train_images])\n",
        "print(\"Histograma de treino criado.\")\n",
        "\n",
        "# Liberar memória\n",
        "del train_images\n",
        "gc.collect()\n",
        "print(\"train_images removido da memória.\")\n",
        "\n",
        "# Converter imagens de validação em histogramas\n",
        "print(\"\\nConvertendo imagens de validação...\")\n",
        "val_histograms = np.array([image_to_histogram(img, bin_edges) for img in val_images])\n",
        "print(\"Histograma de validação criado.\")\n",
        "\n",
        "# Liberar memória\n",
        "del val_images\n",
        "gc.collect()\n",
        "print(\"val_images removido da memória.\")\n",
        "\n",
        "# Salvar histogramas\n",
        "print(\"\\nSalvando histogramas em disco...\")\n",
        "np.save(os.path.join(ros_dir, \"train_histograms_ros.npy\"), train_histograms)\n",
        "np.save(os.path.join(ros_dir, \"val_histograms_ros.npy\"), val_histograms)\n",
        "print(\"Histogramas salvos com sucesso.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3953fc9a-d986-47e1-8872-213f8518cdb7",
      "metadata": {
        "id": "3953fc9a-d986-47e1-8872-213f8518cdb7"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04a46294-7dfd-46c0-9146-d53220c58dbb",
      "metadata": {
        "id": "04a46294-7dfd-46c0-9146-d53220c58dbb"
      },
      "source": [
        "### Train CubeSatNet DNN model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94c005cb-a745-4594-8aab-15b67e0fcdac",
      "metadata": {
        "id": "94c005cb-a745-4594-8aab-15b67e0fcdac"
      },
      "source": [
        "We will define and train a Dense Neural Network (DNN) model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W6lFGlurS5az",
      "metadata": {
        "id": "W6lFGlurS5az"
      },
      "source": [
        "PARA GRAFICOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xg1Ou7MZx_KY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg1Ou7MZx_KY",
        "outputId": "7801e1c2-c408-4247-cd56-7ee55d42f4ab"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Caminho base\n",
        "ros_dir = '/content/drive/MyDrive/cubsat/RandomOversampling'\n",
        "\n",
        "# Carregar histogramas e rótulos\n",
        "train_histograms = np.load(os.path.join(ros_dir, \"train_histograms_ros.npy\"))\n",
        "val_histograms = np.load(os.path.join(ros_dir, \"val_histograms_ros.npy\"))\n",
        "train_labels = np.load(os.path.join(ros_dir, \"train_labels_ros.npy\"))\n",
        "val_labels = np.load(os.path.join(ros_dir, \"val_labels_ros.npy\"))\n",
        "\n",
        "# Número de classes (assumindo 5, como no seu código original)\n",
        "n_classes = len(np.unique(train_labels))\n",
        "\n",
        "# One-hot encoding dos rótulos\n",
        "train_labels_cat = to_categorical(train_labels, num_classes=n_classes)\n",
        "val_labels_cat = to_categorical(val_labels, num_classes=n_classes)\n",
        "\n",
        "# Definir o modelo\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(train_histograms.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(n_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compilar o modelo\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Treinamento com early stopping\n",
        "history = model.fit(\n",
        "    train_histograms, train_labels_cat,\n",
        "    validation_data=(val_histograms, val_labels_cat),\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    callbacks=[EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)]\n",
        ")\n",
        "\n",
        "# Avaliar o modelo\n",
        "loss, acc = model.evaluate(val_histograms, val_labels_cat, verbose=0)\n",
        "print(f\"\\nAcurácia final na validação: {acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9-mbsloTE3M",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c9-mbsloTE3M",
        "outputId": "bcb75a23-8c32-4532-8690-de383383a123"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "# === Plot da Acurácia ===\n",
        "fig_acc = go.Figure()\n",
        "fig_acc.add_trace(go.Scatter(y=history.history['accuracy'], mode='lines+markers', name='Treino'))\n",
        "fig_acc.add_trace(go.Scatter(y=history.history['val_accuracy'], mode='lines+markers', name='Validação'))\n",
        "fig_acc.update_layout(\n",
        "    title=\"Evolução da Acurácia\",\n",
        "    xaxis_title=\"Época\",\n",
        "    yaxis_title=\"Acurácia\",\n",
        "    yaxis=dict(range=[0, 1]),\n",
        "    template=\"plotly_white\"\n",
        ")\n",
        "fig_acc.show()\n",
        "\n",
        "# === Plot da Loss ===\n",
        "fig_loss = go.Figure()\n",
        "fig_loss.add_trace(go.Scatter(y=history.history['loss'], mode='lines+markers', name='Treino'))\n",
        "fig_loss.add_trace(go.Scatter(y=history.history['val_loss'], mode='lines+markers', name='Validação'))\n",
        "fig_loss.update_layout(\n",
        "    title=\"Evolução da Loss\",\n",
        "    xaxis_title=\"Época\",\n",
        "    yaxis_title=\"Loss\",\n",
        "    yaxis=dict(range=[0, 1]),\n",
        "    template=\"plotly_white\"\n",
        ")\n",
        "fig_loss.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5S2-_qcnyENf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5S2-_qcnyENf",
        "outputId": "76bf7e93-9324-4c1b-fb4d-bf02c3cb4af4"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "# === Plot da Acurácia ===\n",
        "fig_acc = go.Figure()\n",
        "fig_acc.add_trace(go.Scatter(y=history.history['accuracy'], mode='lines+markers', name='Treino'))\n",
        "fig_acc.add_trace(go.Scatter(y=history.history['val_accuracy'], mode='lines+markers', name='Validação'))\n",
        "fig_acc.update_layout(\n",
        "    title=\"Evolução da Acurácia\",\n",
        "    xaxis_title=\"Época\",\n",
        "    yaxis_title=\"Acurácia\",\n",
        "    template=\"plotly_white\"\n",
        ")\n",
        "fig_acc.show()\n",
        "\n",
        "# === Plot da Loss ===\n",
        "fig_loss = go.Figure()\n",
        "fig_loss.add_trace(go.Scatter(y=history.history['loss'], mode='lines+markers', name='Treino'))\n",
        "fig_loss.add_trace(go.Scatter(y=history.history['val_loss'], mode='lines+markers', name='Validação'))\n",
        "fig_loss.update_layout(\n",
        "    title=\"Evolução da Loss\",\n",
        "    xaxis_title=\"Época\",\n",
        "    yaxis_title=\"Loss\",\n",
        "    template=\"plotly_white\"\n",
        ")\n",
        "fig_loss.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Z1d9zgBy13T0",
      "metadata": {
        "id": "Z1d9zgBy13T0"
      },
      "source": [
        "Otimizando o codigo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0YFHwePt7nRQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0YFHwePt7nRQ",
        "outputId": "ae9a4af6-d70d-42b9-d6a2-3cfe47300b0f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from kerastuner.tuners import RandomSearch\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "\n",
        "# Caminho base\n",
        "ros_dir = '/content/drive/MyDrive/cubsat/RandomOversampling'\n",
        "\n",
        "# 1. Função para carregar os dados\n",
        "def carregar_dados(ros_dir):\n",
        "    train_X = np.load(os.path.join(ros_dir, \"train_histograms_ros.npy\"))\n",
        "    val_X = np.load(os.path.join(ros_dir, \"val_histograms_ros.npy\"))\n",
        "    train_y = np.load(os.path.join(ros_dir, \"train_labels_ros.npy\"))\n",
        "    val_y = np.load(os.path.join(ros_dir, \"val_labels_ros.npy\"))\n",
        "    return train_X, val_X, train_y, val_y\n",
        "\n",
        "# 2. Função de criação do modelo com busca de hiperparâmetros\n",
        "def construir_modelo_hp(hp):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(\n",
        "        units=hp.Int('units_hidden1', min_value=64, max_value=256, step=32),\n",
        "        activation=hp.Choice('activation1', ['relu', 'tanh']),\n",
        "        input_shape=(train_X.shape[1],)\n",
        "    ))\n",
        "\n",
        "    # Dropout opcional\n",
        "    model.add(Dropout(hp.Float('dropout1', min_value=0.0, max_value=0.5, step=0.1)))\n",
        "\n",
        "    model.add(Dense(\n",
        "        units=hp.Int('units_hidden2', min_value=32, max_value=128, step=32),\n",
        "        activation=hp.Choice('activation2', ['relu', 'tanh'])\n",
        "    ))\n",
        "\n",
        "    model.add(Dropout(hp.Float('dropout2', min_value=0.0, max_value=0.5, step=0.1)))\n",
        "\n",
        "    model.add(Dense(n_classes, activation='softmax'))\n",
        "\n",
        "    # Escolher otimizador\n",
        "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
        "    optimizer_name = hp.Choice('optimizer', ['adam', 'sgd', 'rmsprop'])\n",
        "    if optimizer_name == 'adam':\n",
        "        optimizer = Adam(learning_rate=learning_rate)\n",
        "    elif optimizer_name == 'sgd':\n",
        "        optimizer = SGD(learning_rate=learning_rate)\n",
        "    else:\n",
        "        optimizer = RMSprop(learning_rate=learning_rate)\n",
        "\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# 3. Plot com Plotly\n",
        "def plotar_historico_plotly(history):\n",
        "    fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Acurácia\", \"Loss\"))\n",
        "\n",
        "    fig.add_trace(go.Scatter(\n",
        "        y=history.history['accuracy'], mode='lines+markers', name='Acurácia Treino'\n",
        "    ), row=1, col=1)\n",
        "\n",
        "    fig.add_trace(go.Scatter(\n",
        "        y=history.history['val_accuracy'], mode='lines+markers', name='Acurácia Validação'\n",
        "    ), row=1, col=1)\n",
        "\n",
        "    fig.add_trace(go.Scatter(\n",
        "        y=history.history['loss'], mode='lines+markers', name='Loss Treino'\n",
        "    ), row=1, col=2)\n",
        "\n",
        "    fig.add_trace(go.Scatter(\n",
        "        y=history.history['val_loss'], mode='lines+markers', name='Loss Validação'\n",
        "    ), row=1, col=2)\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=\"Histórico de Treinamento\",\n",
        "        xaxis_title=\"Época\",\n",
        "        xaxis2_title=\"Época\",\n",
        "        yaxis_title=\"Acurácia\",\n",
        "        yaxis2_title=\"Loss\",\n",
        "        template=\"plotly_white\",\n",
        "        width=1000,\n",
        "        height=400\n",
        "    )\n",
        "    fig.show()\n",
        "\n",
        "# 4. Executar pipeline\n",
        "train_X, val_X, train_y_raw, val_y_raw = carregar_dados(ros_dir)\n",
        "n_classes = len(np.unique(train_y_raw))\n",
        "train_y = to_categorical(train_y_raw, num_classes=n_classes)\n",
        "val_y = to_categorical(val_y_raw, num_classes=n_classes)\n",
        "\n",
        "# 5. Inicializar o tuner\n",
        "tuner = RandomSearch(\n",
        "    construir_modelo_hp,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=20,\n",
        "    executions_per_trial=1,\n",
        "    directory='keras_tuner_dir',\n",
        "    project_name='cubsat_tuning'\n",
        ")\n",
        "\n",
        "tuner.search_space_summary()\n",
        "\n",
        "# 6. Treinar com early stopping\n",
        "callbacks = [EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)]\n",
        "\n",
        "tuner.search(train_X, train_y,\n",
        "             validation_data=(val_X, val_y),\n",
        "             epochs=30,\n",
        "             batch_size=32,\n",
        "             callbacks=callbacks,\n",
        "             verbose=1)\n",
        "\n",
        "# 7. Melhor modelo\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "# 8. Treinar melhor modelo final\n",
        "history = best_model.fit(\n",
        "    train_X, train_y,\n",
        "    validation_data=(val_X, val_y),\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 9. Avaliar e salvar\n",
        "loss, acc = best_model.evaluate(val_X, val_y, verbose=0)\n",
        "print(f\"\\nAcurácia final na validação: {acc:.4f}\")\n",
        "\n",
        "# Salvar melhor modelo\n",
        "modelo_final_path = os.path.join(ros_dir, \"modelo_melhor_otimizado.keras\")\n",
        "best_model.save(modelo_final_path)\n",
        "print(f\"Modelo salvo em: {modelo_final_path}\")\n",
        "\n",
        "# 10. Visualizar histórico\n",
        "plotar_historico_plotly(history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0XscAWyjBW52",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XscAWyjBW52",
        "outputId": "afa63167-22c1-4bff-c57f-5bf5b814d70d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Caminho do modelo salvo\n",
        "modelo_path = '/content/drive/MyDrive/cubsat/RandomOversampling/modelo_melhor_otimizado.keras'\n",
        "\n",
        "# Carregar modelo salvo\n",
        "modelo = load_model(modelo_path)\n",
        "\n",
        "# Recarregar dados\n",
        "ros_dir = '/content/drive/MyDrive/cubsat/RandomOversampling'\n",
        "train_X = np.load(os.path.join(ros_dir, \"train_histograms_ros.npy\"))\n",
        "val_X = np.load(os.path.join(ros_dir, \"val_histograms_ros.npy\"))\n",
        "train_y_raw = np.load(os.path.join(ros_dir, \"train_labels_ros.npy\"))\n",
        "val_y_raw = np.load(os.path.join(ros_dir, \"val_labels_ros.npy\"))\n",
        "\n",
        "# One-hot encoding\n",
        "n_classes = len(np.unique(train_y_raw))\n",
        "train_y = to_categorical(train_y_raw, num_classes=n_classes)\n",
        "val_y = to_categorical(val_y_raw, num_classes=n_classes)\n",
        "\n",
        "# Treinamento por 50 épocas sem early stopping\n",
        "history = modelo.fit(\n",
        "    train_X, train_y,\n",
        "    validation_data=(val_X, val_y),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Salvar o modelo reentreinado\n",
        "novo_path = os.path.join(ros_dir, \"modelo_melhor_otimizado_mais50epocas.keras\")\n",
        "modelo.save(novo_path)\n",
        "print(f\"Modelo reentreinado salvo em: {novo_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HHkYV3OiCPjn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HHkYV3OiCPjn",
        "outputId": "ed6cb9a3-3a81-4ae4-ad79-1367bdda84b2"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "# === Plot da Acurácia ===\n",
        "fig_acc = go.Figure()\n",
        "fig_acc.add_trace(go.Scatter(y=history.history['accuracy'], mode='lines+markers', name='Treino'))\n",
        "fig_acc.add_trace(go.Scatter(y=history.history['val_accuracy'], mode='lines+markers', name='Validação'))\n",
        "fig_acc.update_layout(\n",
        "    title=\"Evolução da Acurácia\",\n",
        "    xaxis_title=\"Época\",\n",
        "    yaxis_title=\"Acurácia\",\n",
        "    yaxis=dict(range=[0, 1]),\n",
        "    template=\"plotly_white\"\n",
        ")\n",
        "fig_acc.show()\n",
        "\n",
        "# === Plot da Loss ===\n",
        "fig_loss = go.Figure()\n",
        "fig_loss.add_trace(go.Scatter(y=history.history['loss'], mode='lines+markers', name='Treino'))\n",
        "fig_loss.add_trace(go.Scatter(y=history.history['val_loss'], mode='lines+markers', name='Validação'))\n",
        "fig_loss.update_layout(\n",
        "    title=\"Evolução da Loss\",\n",
        "    xaxis_title=\"Época\",\n",
        "    yaxis_title=\"Loss\",\n",
        "    yaxis=dict(range=[0, 1]),\n",
        "    template=\"plotly_white\"\n",
        ")\n",
        "fig_loss.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vvtX1sR1zxMM",
      "metadata": {
        "id": "vvtX1sR1zxMM"
      },
      "source": [
        "TESTANDO O MODELO COM O CONJUNTO DE TESTES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pwQfI53dzxBS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "pwQfI53dzxBS",
        "outputId": "29646882-0ef9-4ec4-f075-56f69d509b0c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import plotly.figure_factory as ff\n",
        "\n",
        "# Caminhos\n",
        "ros_dir = '/content/drive/MyDrive/cubsat/RandomOversampling'\n",
        "test_images_path = '/content/drive/MyDrive/cubsat/test_images.npy'\n",
        "test_labels_path = '/content/drive/MyDrive/cubsat/test_labels.npy'\n",
        "bin_edges_path = os.path.join(ros_dir, 'bin_edges_ros.npy')\n",
        "model_path = os.path.join(ros_dir, '/content/drive/MyDrive/cubsat/RandomOversampling/modelo_melhor_otimizado_mais50epocas.keras')\n",
        "\n",
        "# Carregar dados e modelo\n",
        "test_images = np.load(test_images_path)\n",
        "test_labels = np.load(test_labels_path)\n",
        "model = load_model(model_path)\n",
        "bin_edges = np.load(bin_edges_path, allow_pickle=True)\n",
        "\n",
        "# Função de extração de histogramas\n",
        "def image_to_histogram(image, bin_edges):\n",
        "    features = []\n",
        "    for channel in range(3):\n",
        "        pixels = image[:, :, channel].flatten()\n",
        "        hist, _ = np.histogram(pixels, bins=bin_edges[channel])\n",
        "        hist = hist / len(pixels)\n",
        "        features.extend(hist)\n",
        "    return np.array(features)\n",
        "\n",
        "# Converter imagens de teste em histogramas\n",
        "test_histograms = np.array([image_to_histogram(img, bin_edges) for img in test_images])\n",
        "\n",
        "# Obter previsões\n",
        "predictions = model.predict(test_histograms)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Matriz de confusão\n",
        "conf_matrix = confusion_matrix(test_labels, predicted_labels)\n",
        "labels = [f\"Classe {i}\" for i in range(conf_matrix.shape[0])]\n",
        "\n",
        "# Plot com Plotly\n",
        "fig = ff.create_annotated_heatmap(\n",
        "    z=conf_matrix,\n",
        "    x=labels,\n",
        "    y=labels,\n",
        "    colorscale='Blues',\n",
        "    showscale=True\n",
        ")\n",
        "fig.update_layout(\n",
        "    title=\"Matriz de Confusão - Conjunto de Teste\",\n",
        "    xaxis_title=\"Rótulo Previsto\",\n",
        "    yaxis_title=\"Rótulo Verdadeiro\"\n",
        ")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zz3G0gKNzw59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zz3G0gKNzw59",
        "outputId": "b906cfe9-5d6f-45fa-9d80-864847b86868"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Gerar o relatório\n",
        "report = classification_report(test_labels, predicted_labels, digits=4)\n",
        "\n",
        "# Imprimir\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WB8CnOPc1d0t",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WB8CnOPc1d0t",
        "outputId": "72249aa3-11b5-47ad-dec0-fe60a4a1f8d2"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import psutil\n",
        "import tracemalloc\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Início da medição de tempo, memória e CPU\n",
        "start_time = time.time()\n",
        "tracemalloc.start()\n",
        "process = psutil.Process(os.getpid())\n",
        "cpu_percent_start = psutil.cpu_percent(interval=None)\n",
        "\n",
        "# Previsão\n",
        "predictions = model.predict(test_histograms)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Métricas\n",
        "accuracy = accuracy_score(test_labels, predicted_labels)\n",
        "f1 = f1_score(test_labels, predicted_labels, average='weighted')\n",
        "\n",
        "# Término da medição\n",
        "cpu_percent_end = psutil.cpu_percent(interval=None)\n",
        "_, peak_memory = tracemalloc.get_traced_memory()\n",
        "tracemalloc.stop()\n",
        "end_time = time.time()\n",
        "\n",
        "# Tamanho do código do modelo salvo\n",
        "model_file_path = os.path.join(ros_dir, 'dnn_histogram_model.keras')\n",
        "model_code_size = os.path.getsize(model_file_path) / 1024  # em KB\n",
        "\n",
        "# Impressão formatada\n",
        "print(\"Evaluation Time:        {:.4f} seconds\".format(end_time - start_time))\n",
        "print(\"Peak Memory Usage:      {:.2f} MB\".format(peak_memory / (1024 * 1024)))\n",
        "print(\"Average CPU Usage:      {:.2f}%\".format((cpu_percent_start + cpu_percent_end) / 2))\n",
        "print(\"Algorithm Code Size:    {:.2f} KB\".format(model_code_size))\n",
        "print(\"Accuracy:               {:.4f}\".format(accuracy))\n",
        "print(\"F1 Score:               {:.4f}\".format(f1))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
