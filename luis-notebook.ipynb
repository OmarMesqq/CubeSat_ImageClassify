{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "59ed05bb-fe72-45c8-aaf4-f067ee02838e",
      "metadata": {
        "id": "59ed05bb-fe72-45c8-aaf4-f067ee02838e"
      },
      "source": [
        "# Classification using Deep Learning with Histogram data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13095d08-82fa-49dc-8613-e8d4d1320555",
      "metadata": {
        "id": "13095d08-82fa-49dc-8613-e8d4d1320555"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5f39eb1-6f50-40c3-8468-226c0c8ee6b4",
      "metadata": {
        "id": "d5f39eb1-6f50-40c3-8468-226c0c8ee6b4"
      },
      "source": [
        "### Reading the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97f576c9-784f-49d2-bf5d-6978edeb89d4",
      "metadata": {
        "id": "97f576c9-784f-49d2-bf5d-6978edeb89d4"
      },
      "source": [
        "First, we'll load the saved image and label data from the NumPy files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GjekVGlXEFHT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjekVGlXEFHT",
        "outputId": "048a44a4-a01e-4d99-b38a-f60b875a097b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Base path for .npy files\n",
        "base_path = '/content/drive/MyDrive/cubsat'\n",
        "\n",
        "# Loading files with expected names\n",
        "train_images = np.load(f'{base_path}/train_images.npy')\n",
        "train_labels = np.load(f'{base_path}/train_labels.npy')\n",
        "val_images   = np.load(f'{base_path}/val_images.npy')\n",
        "val_labels   = np.load(f'{base_path}/val_labels.npy')\n",
        "\n",
        "# Display shapes for verification\n",
        "print(\"train_images shape:\", train_images.shape)\n",
        "print(\"train_labels shape:\", train_labels.shape)\n",
        "print(\"val_images shape:\", val_images.shape)\n",
        "print(\"val_labels shape:\", val_labels.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03a0d2f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03a0d2f2",
        "outputId": "9b79d46e-1dc7-4ab5-c111-8775ff1a9eff"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Display image information\n",
        "print(\"=== Image Characteristics ===\")\n",
        "print(f\"Shape of training set: {train_images.shape} (Samples, Height, Width, Channels)\")\n",
        "print(f\"Shape of validation set: {val_images.shape} (Samples, Height, Width, Channels)\")\n",
        "\n",
        "# Image resolution (height and width)\n",
        "image_resolution = train_images.shape[1:3]  # Taking height and width dimensions\n",
        "print(f\"Image resolution: {image_resolution} pixels\")\n",
        "\n",
        "# Total number of images\n",
        "print(f\"Number of images in training set: {train_images.shape[0]}\")\n",
        "print(f\"Number of images in validation set: {val_images.shape[0]}\")\n",
        "\n",
        "# Data type of the images\n",
        "print(f\"Data type of images: {train_images.dtype}\")\n",
        "\n",
        "# Range of values in the images\n",
        "print(f\"Minimum and maximum values in training dataset: {train_images.min()} to {train_images.max()}\")\n",
        "\n",
        "# Display some unique labels and their counts in the training set\n",
        "unique_labels_train, counts_train = np.unique(train_labels, return_counts=True)\n",
        "print(\"\\n=== Class Distribution in Training Set ===\")\n",
        "for label, count in zip(unique_labels_train, counts_train):\n",
        "    print(f\"Class {label}: {count} samples\")\n",
        "\n",
        "# Display some unique labels and their counts in the validation set\n",
        "unique_labels_val, counts_val = np.unique(val_labels, return_counts=True)\n",
        "print(\"\\n=== Class Distribution in Validation Set ===\")\n",
        "for label, count in zip(unique_labels_val, counts_val):\n",
        "    print(f\"Class {label}: {count} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad3d8f2e",
      "metadata": {
        "id": "ad3d8f2e"
      },
      "source": [
        "## Combining training and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iZphsFEdJE7E",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZphsFEdJE7E",
        "outputId": "024f8884-578c-4ca5-8e2b-3a20dc1c3be6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# New directory where combined files will be saved\n",
        "cached_data_dir = '/content/drive/MyDrive/cubsat'\n",
        "os.makedirs(cached_data_dir, exist_ok=True)\n",
        "\n",
        "# Paths for combined .npy files\n",
        "all_images_path = os.path.join(cached_data_dir, 'all_images.npy')\n",
        "all_labels_path = os.path.join(cached_data_dir, 'all_labels.npy')\n",
        "\n",
        "# Check if files already exist\n",
        "if os.path.exists(all_images_path) and os.path.exists(all_labels_path):\n",
        "    print(\"Combined files already exist. Loading from disk...\")\n",
        "    all_images = np.load(all_images_path)\n",
        "    all_labels = np.load(all_labels_path)\n",
        "else:\n",
        "    print(\"Combined files do not exist. Concatenating data...\")\n",
        "    # Concatenate training and validation sets\n",
        "    all_images = np.concatenate((train_images, val_images), axis=0)\n",
        "    all_labels = np.concatenate((train_labels, val_labels), axis=0)\n",
        "\n",
        "    # Save combined files\n",
        "    np.save(all_images_path, all_images)\n",
        "    np.save(all_labels_path, all_labels)\n",
        "    print(\"Data concatenated and saved.\")\n",
        "\n",
        "# Display image information\n",
        "print(\"=== Image Characteristics ===\")\n",
        "print(f\"Shape of combined dataset: {all_images.shape} (Samples, Height, Width, Channels)\")\n",
        "print(f\"Image resolution: {all_images.shape[1:3]} pixels\")\n",
        "print(f\"Total number of images: {all_images.shape[0]}\")\n",
        "print(f\"Data type of images: {all_images.dtype}\")\n",
        "print(f\"Minimum and maximum values in combined dataset: {all_images.min()} to {all_images.max()}\")\n",
        "\n",
        "# Display class distribution\n",
        "unique_labels, counts = np.unique(all_labels, return_counts=True)\n",
        "print(\"\\n=== Class Distribution in Combined Dataset ===\")\n",
        "for label, count in zip(unique_labels, counts):\n",
        "    print(f\"Class {label}: {count} samples\")\n",
        "\n",
        "print(f\"\\nData saved in: {cached_data_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Gd2WuAMwJV0E",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gd2WuAMwJV0E",
        "outputId": "bac14b09-f6a5-4232-ed1a-7e146550503e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# File path\n",
        "all_images_path = '/content/drive/MyDrive/cubsat/all_images.npy'\n",
        "\n",
        "# Load .npy file\n",
        "all_images = np.load(all_images_path)\n",
        "\n",
        "# Check data shape and type\n",
        "print(\"Shape:\", all_images.shape)\n",
        "print(\"Dtype:\", all_images.dtype)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ooy80GTtNI-r",
      "metadata": {
        "id": "Ooy80GTtNI-r"
      },
      "source": [
        "PLOTTING RANDOM SAMPLES FROM EACH CLASS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YDOsYKeUKMl4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YDOsYKeUKMl4",
        "outputId": "60127ac2-43cf-4235-c8fb-868ecc79224c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import random\n",
        "\n",
        "# File paths\n",
        "all_images_path = '/content/drive/MyDrive/cubsat/all_images.npy'\n",
        "all_labels_path = '/content/drive/MyDrive/cubsat/all_labels.npy'\n",
        "\n",
        "# Load data\n",
        "all_images = np.load(all_images_path)\n",
        "all_labels = np.load(all_labels_path)\n",
        "\n",
        "# Identify unique classes\n",
        "unique_classes = np.unique(all_labels)\n",
        "\n",
        "# Plot a random image from each class\n",
        "for cls in unique_classes:\n",
        "    # Indices of images in current class\n",
        "    indices = np.where(all_labels == cls)[0]\n",
        "    # Choose random index\n",
        "    random_index = random.choice(indices)\n",
        "    # Select image\n",
        "    image = all_images[random_index]\n",
        "\n",
        "    # Plot with Plotly\n",
        "    fig = px.imshow(image.astype(np.uint8))\n",
        "    fig.update_layout(title=f\"Class {cls}\")\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a739cd9",
      "metadata": {
        "id": "3a739cd9"
      },
      "source": [
        "## APPLYING DATA AUGMENTATION FOR CLASS BALANCING"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q6emBRccN1pN",
      "metadata": {
        "id": "q6emBRccN1pN"
      },
      "source": [
        " Show a random image with its class (after BALANCING):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jY5UwembTEP3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jY5UwembTEP3",
        "outputId": "48133c6f-70e0-49f5-b34f-95eb1a7207af"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Original files directory\n",
        "base_dir = '/content/drive/MyDrive/cubsat'\n",
        "all_images_path = os.path.join(base_dir, 'all_images.npy')\n",
        "all_labels_path = os.path.join(base_dir, 'all_labels.npy')\n",
        "\n",
        "# Directory for balanced data\n",
        "ros_dir = os.path.join(base_dir, \"RandomOversampling\")\n",
        "os.makedirs(ros_dir, exist_ok=True)\n",
        "\n",
        "# Paths for balanced files\n",
        "ros_images_path = os.path.join(ros_dir, \"all_images_ros.npy\")\n",
        "ros_labels_path = os.path.join(ros_dir, \"all_labels_ros.npy\")\n",
        "\n",
        "# Load original data\n",
        "all_images = np.load(all_images_path)\n",
        "all_labels = np.load(all_labels_path)\n",
        "\n",
        "# Check if files already exist\n",
        "if os.path.exists(ros_images_path) and os.path.exists(ros_labels_path):\n",
        "    print(\"Random Oversampled files already exist. Loading from disk...\")\n",
        "    X_resampled = np.load(ros_images_path)\n",
        "    y_resampled = np.load(ros_labels_path)\n",
        "else:\n",
        "    print(\"Applying Random Oversampling...\")\n",
        "\n",
        "    # Temporary flattening for oversampling\n",
        "    num_samples, height, width, channels = all_images.shape\n",
        "    all_images_flat = all_images.reshape(num_samples, -1)\n",
        "\n",
        "    # Apply RandomOverSampler\n",
        "    ros = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
        "    X_resampled_flat, y_resampled = ros.fit_resample(all_images_flat, all_labels)\n",
        "\n",
        "    # Restore original shape\n",
        "    X_resampled = X_resampled_flat.reshape(-1, height, width, channels)\n",
        "\n",
        "    # Shuffle\n",
        "    X_resampled, y_resampled = shuffle(X_resampled, y_resampled, random_state=42)\n",
        "\n",
        "    # Save\n",
        "    np.save(ros_images_path, X_resampled)\n",
        "    np.save(ros_labels_path, y_resampled)\n",
        "    print(\"Random Oversampling applied and saved.\")\n",
        "\n",
        "# Display class distribution\n",
        "unique_labels, counts = np.unique(y_resampled, return_counts=True)\n",
        "print(\"\\n=== Class Distribution after Random Oversampling ===\")\n",
        "for label, count in zip(unique_labels, counts):\n",
        "    print(f\"Class {label}: {count} samples\")\n",
        "\n",
        "print(f\"\\nBalanced data saved in: {ros_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gWALjkMxTGp_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "gWALjkMxTGp_",
        "outputId": "c16d50d6-a8e3-4dc1-c02b-ef1a62675591"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import plotly.express as px\n",
        "import random\n",
        "\n",
        "# Paths\n",
        "ros_dir = '/content/drive/MyDrive/cubsat/RandomOversampling'\n",
        "ros_images_path = os.path.join(ros_dir, \"all_images_ros.npy\")\n",
        "ros_labels_path = os.path.join(ros_dir, \"all_labels_ros.npy\")\n",
        "\n",
        "# Load data\n",
        "images = np.load(ros_images_path)\n",
        "labels = np.load(ros_labels_path)\n",
        "\n",
        "# Identify unique classes\n",
        "unique_classes = np.unique(labels)\n",
        "\n",
        "# Create subplots with plotly\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "n_classes = len(unique_classes)\n",
        "fig = make_subplots(rows=1, cols=n_classes, subplot_titles=[f\"Class {cls}\" for cls in unique_classes])\n",
        "\n",
        "# Add a random image from each class\n",
        "for idx, cls in enumerate(unique_classes):\n",
        "    indices = np.where(labels == cls)[0]\n",
        "    selected_idx = random.choice(indices)\n",
        "    img = images[selected_idx]\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Image(z=img.astype(np.uint8)),\n",
        "        row=1, col=idx+1\n",
        "    )\n",
        "\n",
        "# Layout\n",
        "fig.update_layout(height=300, width=200 * n_classes, title_text=\"Random Example from Each Class (Balanced Dataset)\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9849d30b",
      "metadata": {
        "id": "9849d30b"
      },
      "source": [
        "## Splitting combined set in proportion 80% for training and 20% for validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u5BFldJWhFpB",
      "metadata": {
        "id": "u5BFldJWhFpB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "\n",
        "# Paths\n",
        "ros_dir = '/content/drive/MyDrive/cubsat/RandomOversampling'\n",
        "ros_images_path = os.path.join(ros_dir, \"all_images_ros.npy\")\n",
        "ros_labels_path = os.path.join(ros_dir, \"all_labels_ros.npy\")\n",
        "\n",
        "# Load data\n",
        "images = np.load(ros_images_path)\n",
        "labels = np.load(ros_labels_path)\n",
        "\n",
        "# Identify unique classes\n",
        "unique_classes = np.unique(labels)\n",
        "\n",
        "# Lists to store split data\n",
        "train_images, train_labels = [], []\n",
        "val_images, val_labels = [], []\n",
        "\n",
        "# Ensure reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Split data while maintaining balance\n",
        "for cls in unique_classes:\n",
        "    indices = np.where(labels == cls)[0]\n",
        "    random.shuffle(indices.tolist())\n",
        "\n",
        "    split_idx = int(0.8 * len(indices))\n",
        "    train_idx = indices[:split_idx]\n",
        "    val_idx = indices[split_idx:]\n",
        "\n",
        "    train_images.append(images[train_idx])\n",
        "    train_labels.append(labels[train_idx])\n",
        "    val_images.append(images[val_idx])\n",
        "    val_labels.append(labels[val_idx])\n",
        "\n",
        "# Concatenate results\n",
        "train_images = np.concatenate(train_images, axis=0)\n",
        "train_labels = np.concatenate(train_labels, axis=0)\n",
        "val_images = np.concatenate(val_images, axis=0)\n",
        "val_labels = np.concatenate(val_labels, axis=0)\n",
        "\n",
        "# Save files\n",
        "np.save(os.path.join(ros_dir, \"train_images_ros.npy\"), train_images)\n",
        "np.save(os.path.join(ros_dir, \"train_labels_ros.npy\"), train_labels)\n",
        "np.save(os.path.join(ros_dir, \"val_images_ros.npy\"), val_images)\n",
        "np.save(os.path.join(ros_dir, \"val_labels_ros.npy\"), val_labels)\n",
        "\n",
        "print(\"Sets saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3MWDUt77sn3h",
      "metadata": {
        "id": "3MWDUt77sn3h"
      },
      "source": [
        "GENERATING RANDOM IMAGES OF EACH CLASS PER SET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xToqOV1ShIBY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "xToqOV1ShIBY",
        "outputId": "a49b389a-ef8a-4b84-f54d-a97a0ae51ecd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Path\n",
        "ros_dir = '/content/drive/MyDrive/cubsat/RandomOversampling'\n",
        "train_images = np.load(os.path.join(ros_dir, \"train_images_ros.npy\"))\n",
        "train_labels = np.load(os.path.join(ros_dir, \"train_labels_ros.npy\"))\n",
        "\n",
        "# Unique classes\n",
        "unique_classes = np.unique(train_labels)\n",
        "n_classes = len(unique_classes)\n",
        "\n",
        "# Create subplots\n",
        "fig = make_subplots(rows=1, cols=n_classes, subplot_titles=[f\"Class {cls}\" for cls in unique_classes])\n",
        "\n",
        "# Add random image from each class\n",
        "for idx, cls in enumerate(unique_classes):\n",
        "    indices = np.where(train_labels == cls)[0]\n",
        "    selected_idx = random.choice(indices)\n",
        "    img = train_images[selected_idx]\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Image(z=img.astype(np.uint8)),\n",
        "        row=1, col=idx+1\n",
        "    )\n",
        "\n",
        "fig.update_layout(height=300, width=200 * n_classes, title_text=\"Training: Random image per class\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GcPVuZVthH3-",
      "metadata": {
        "id": "GcPVuZVthH3-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Path\n",
        "ros_dir = '/content/drive/MyDrive/cubsat/RandomOversampling'\n",
        "val_images = np.load(os.path.join(ros_dir, \"val_images_ros.npy\"))\n",
        "val_labels = np.load(os.path.join(ros_dir, \"val_labels_ros.npy\"))\n",
        "\n",
        "# Unique classes\n",
        "unique_classes = np.unique(val_labels)\n",
        "n_classes = len(unique_classes)\n",
        "\n",
        "# Create subplots\n",
        "fig = make_subplots(rows=1, cols=n_classes, subplot_titles=[f\"Class {cls}\" for cls in unique_classes])\n",
        "\n",
        "# Add random image from each class\n",
        "for idx, cls in enumerate(unique_classes):\n",
        "    indices = np.where(val_labels == cls)[0]\n",
        "    selected_idx = random.choice(indices)\n",
        "    img = val_images[selected_idx]\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Image(z=img.astype(np.uint8)),\n",
        "        row=1, col=idx+1\n",
        "    )\n",
        "\n",
        "fig.update_layout(height=300, width=200 * n_classes, title_text=\"Validation: Random image per class\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r2jYzsxdqiW7",
      "metadata": {
        "id": "r2jYzsxdqiW7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import collections\n",
        "\n",
        "# Path\n",
        "ros_dir = '/content/drive/MyDrive/cubsat/RandomOversampling'\n",
        "\n",
        "# Load labels\n",
        "train_labels = np.load(os.path.join(ros_dir, \"train_labels_ros.npy\"))\n",
        "val_labels = np.load(os.path.join(ros_dir, \"val_labels_ros.npy\"))\n",
        "\n",
        "# Count classes\n",
        "train_counts = collections.Counter(train_labels)\n",
        "val_counts = collections.Counter(val_labels)\n",
        "\n",
        "# Display results\n",
        "print(\"Class distribution in TRAINING set:\")\n",
        "for cls, count in sorted(train_counts.items()):\n",
        "    print(f\"Class {cls}: {count} images\")\n",
        "\n",
        "print(\"\\nClass distribution in VALIDATION set:\")\n",
        "for cls, count in sorted(val_counts.items()):\n",
        "    print(f\"Class {cls}: {count} images\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UE21UV_Ht9hB",
      "metadata": {
        "id": "UE21UV_Ht9hB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Updated paths\n",
        "ros_dir = '/content/drive/MyDrive/cubsat/RandomOversampling'\n",
        "bin_edges_path = os.path.join(ros_dir, 'bin_edges_ros.npy')\n",
        "\n",
        "# Load training and validation data\n",
        "train_images = np.load(os.path.join(ros_dir, 'train_images_ros.npy'))\n",
        "train_labels = np.load(os.path.join(ros_dir, 'train_labels_ros.npy'))\n",
        "val_images = np.load(os.path.join(ros_dir, 'val_images_ros.npy'))\n",
        "val_labels = np.load(os.path.join(ros_dir, 'val_labels_ros.npy'))\n",
        "\n",
        "# Check if bin_edges file exists\n",
        "if os.path.exists(bin_edges_path):\n",
        "    print(\"bin_edges file already exists. Loading from disk...\")\n",
        "    bin_edges = np.load(bin_edges_path, allow_pickle=True)\n",
        "else:\n",
        "    print(\"bin_edges file not found. Calculating quantiles...\")\n",
        "\n",
        "    # Define quantile levels (deciles -> 10 intervals)\n",
        "    quantile_levels = np.linspace(0, 1, num=11)\n",
        "    bin_edges = []\n",
        "\n",
        "    # Calculate quantiles for each channel (R, G, B)\n",
        "    for channel in range(3):\n",
        "        channel_pixels = train_images[:, :, :, channel].flatten()\n",
        "        edges = np.quantile(channel_pixels, quantile_levels)\n",
        "        bin_edges.append(edges)\n",
        "\n",
        "    # Save bin_edges\n",
        "    np.save(bin_edges_path, bin_edges)\n",
        "    print(\"Quantiles calculated and bin_edges saved.\")\n",
        "\n",
        "# Display bin_edges\n",
        "print(\"\\n=== Bin Edges per Channel ===\")\n",
        "for channel, edges in enumerate(bin_edges):\n",
        "    print(f\"Channel {channel} (R={channel==0}, G={channel==1}, B={channel==2}): {edges}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BrDE20wpv-N_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrDE20wpv-N_",
        "outputId": "f3f8cfcf-b832-4f54-96aa-069bc48b199c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import gc\n",
        "\n",
        "# Base path\n",
        "ros_dir = '/content/drive/MyDrive/cubsat/RandomOversampling'\n",
        "\n",
        "# File paths\n",
        "train_images_path = os.path.join(ros_dir, 'train_images_ros.npy')\n",
        "train_labels_path = os.path.join(ros_dir, 'train_labels_ros.npy')\n",
        "val_images_path = os.path.join(ros_dir, 'val_images_ros.npy')\n",
        "val_labels_path = os.path.join(ros_dir, 'val_labels_ros.npy')\n",
        "bin_edges_path = os.path.join(ros_dir, 'bin_edges_ros.npy')\n",
        "\n",
        "# Load data\n",
        "print(\"Loading training and validation data...\")\n",
        "train_images = np.load(train_images_path)\n",
        "train_labels = np.load(train_labels_path)\n",
        "val_images = np.load(val_images_path)\n",
        "val_labels = np.load(val_labels_path)\n",
        "\n",
        "# Check bin_edges existence\n",
        "if os.path.exists(bin_edges_path):\n",
        "    print(\"bin_edges_ros.npy already exists. Loading...\")\n",
        "    bin_edges = np.load(bin_edges_path, allow_pickle=True)\n",
        "else:\n",
        "    print(\"Calculating bin_edges (quantiles)...\")\n",
        "    quantile_levels = np.linspace(0, 1, num=11)  # deciles\n",
        "    bin_edges = []\n",
        "    for channel in range(3):  # R, G, B\n",
        "        pixels = train_images[:, :, :, channel].flatten()\n",
        "        edges = np.quantile(pixels, quantile_levels)\n",
        "        bin_edges.append(edges)\n",
        "    np.save(bin_edges_path, bin_edges)\n",
        "    print(\"bin_edges saved to disk.\")\n",
        "\n",
        "# Display bin_edges (optional)\n",
        "print(\"\\n=== Bin Edges per Channel ===\")\n",
        "for ch, edges in enumerate(bin_edges):\n",
        "    print(f\"Channel {ch}: {edges}\")\n",
        "\n",
        "# Function to convert image to normalized histogram\n",
        "def image_to_histogram(image, bin_edges):\n",
        "    features = []\n",
        "    for channel in range(3):  # R, G, B\n",
        "        pixels = image[:, :, channel].flatten()\n",
        "        hist, _ = np.histogram(pixels, bins=bin_edges[channel])\n",
        "        hist = hist / len(pixels)  # Normalize to proportion\n",
        "        features.extend(hist)\n",
        "    return np.array(features)\n",
        "\n",
        "# Convert training images to histograms\n",
        "print(\"\\nConverting training images...\")\n",
        "train_histograms = np.array([image_to_histogram(img, bin_edges) for img in train_images])\n",
        "print(\"Training histogram created.\")\n",
        "\n",
        "# Free memory\n",
        "del train_images\n",
        "gc.collect()\n",
        "print(\"train_images removed from memory.\")\n",
        "\n",
        "# Convert validation images to histograms\n",
        "print(\"\\nConverting validation images...\")\n",
        "val_histograms = np.array([image_to_histogram(img, bin_edges) for img in val_images])\n",
        "print(\"Validation histogram created.\")\n",
        "\n",
        "# Free memory\n",
        "del val_images\n",
        "gc.collect()\n",
        "print(\"val_images removed from memory.\")\n",
        "\n",
        "# Save histograms\n",
        "print(\"\\nSaving histograms to disk...\")\n",
        "np.save(os.path.join(ros_dir, \"train_histograms_ros.npy\"), train_histograms)\n",
        "np.save(os.path.join(ros_dir, \"val_histograms_ros.npy\"), val_histograms)\n",
        "print(\"Histograms saved successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3953fc9a-d986-47e1-8872-213f8518cdb7",
      "metadata": {
        "id": "3953fc9a-d986-47e1-8872-213f8518cdb7"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04a46294-7dfd-46c0-9146-d53220c58dbb",
      "metadata": {
        "id": "04a46294-7dfd-46c0-9146-d53220c58dbb"
      },
      "source": [
        "### Train CubeSatNet DNN model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94c005cb-a745-4594-8aab-15b67e0fcdac",
      "metadata": {
        "id": "94c005cb-a745-4594-8aab-15b67e0fcdac"
      },
      "source": [
        "We will define and train a Dense Neural Network (DNN) model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W6lFGlurS5az",
      "metadata": {
        "id": "W6lFGlurS5az"
      },
      "source": [
        "FOR PLOTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xg1Ou7MZx_KY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg1Ou7MZx_KY",
        "outputId": "7801e1c2-c408-4247-cd56-7ee55d42f4ab"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Base path\n",
        "ros_dir = '/content/drive/MyDrive/cubsat/RandomOversampling'\n",
        "\n",
        "# Load histograms and labels\n",
        "train_histograms = np.load(os.path.join(ros_dir, \"train_histograms_ros.npy\"))\n",
        "val_histograms = np.load(os.path.join(ros_dir, \"val_histograms_ros.npy\"))\n",
        "train_labels = np.load(os.path.join(ros_dir, \"train_labels_ros.npy\"))\n",
        "val_labels = np.load(os.path.join(ros_dir, \"val_labels_ros.npy\"))\n",
        "\n",
        "# Number of classes (assuming 5 as in original code)\n",
        "n_classes = len(np.unique(train_labels))\n",
        "\n",
        "# One-hot encode labels\n",
        "train_labels_cat = to_categorical(train_labels, num_classes=n_classes)\n",
        "val_labels_cat = to_categorical(val_labels, num_classes=n_classes)\n",
        "\n",
        "# Define model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(train_histograms.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(n_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Training with early stopping\n",
        "history = model.fit(\n",
        "    train_histograms, train_labels_cat,\n",
        "    validation_data=(val_histograms, val_labels_cat),\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    callbacks=[EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)]\n",
        ")\n",
        "\n",
        "# Evaluate model\n",
        "loss, acc = model.evaluate(val_histograms, val_labels_cat, verbose=0)\n",
        "print(f\"\\nFinal validation accuracy: {acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9-mbsloTE3M",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c9-mbsloTE3M",
        "outputId": "bcb75a23-8c32-4532-8690-de383383a123"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "# === Accuracy Plot ===\n",
        "fig_acc = go.Figure()\n",
        "fig_acc.add_trace(go.Scatter(y=history.history['accuracy'], mode='lines+markers', name='Train'))\n",
        "fig_acc.add_trace(go.Scatter(y=history.history['val_accuracy'], mode='lines+markers', name='Validation'))\n",
        "fig_acc.update_layout(\n",
        "    title=\"Accuracy Evolution\",\n",
        "    xaxis_title=\"Epoch\",\n",
        "    yaxis_title=\"Accuracy\",\n",
        "    yaxis=dict(range=[0, 1]),\n",
        "    template=\"plotly_white\"\n",
        ")\n",
        "fig_acc.show()\n",
        "\n",
        "# === Loss Plot ===\n",
        "fig_loss = go.Figure()\n",
        "fig_loss.add_trace(go.Scatter(y=history.history['loss'], mode='lines+markers', name='Train'))\n",
        "fig_loss.add_trace(go.Scatter(y=history.history['val_loss'], mode='lines+markers', name='Validation'))\n",
        "fig_loss.update_layout(\n",
        "    title=\"Loss Evolution\",\n",
        "    xaxis_title=\"Epoch\",\n",
        "    yaxis_title=\"Loss\",\n",
        "    yaxis=dict(range=[0, 1]),\n",
        "    template=\"plotly_white\"\n",
        ")\n",
        "fig_loss.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5S2-_qcnyENf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5S2-_qcnyENf",
        "outputId": "76bf7e93-9324-4c1b-fb4d-bf02c3cb4af4"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "# === Accuracy Plot ===\n",
        "fig_acc = go.Figure()\n",
        "fig_acc.add_trace(go.Scatter(y=history.history['accuracy'], mode='lines+markers', name='Train'))\n",
        "fig_acc.add_trace(go.Scatter(y=history.history['val_accuracy'], mode='lines+markers', name='Validation'))\n",
        "fig_acc.update_layout(\n",
        "    title=\"Accuracy Evolution\",\n",
        "    xaxis_title=\"Epoch\",\n",
        "    yaxis_title=\"Accuracy\",\n",
        "    template=\"plotly_white\"\n",
        ")\n",
        "fig_acc.show()\n",
        "\n",
        "# === Loss Plot ===\n",
        "fig_loss = go.Figure()\n",
        "fig_loss.add_trace(go.Scatter(y=history.history['loss'], mode='lines+markers', name='Train'))\n",
        "fig_loss.add_trace(go.Scatter(y=history.history['val_loss'], mode='lines+markers', name='Validation'))\n",
        "fig_loss.update_layout(\n",
        "    title=\"Loss Evolution\",\n",
        "    xaxis_title=\"Epoch\",\n",
        "    yaxis_title=\"Loss\",\n",
        "    template=\"plotly_white\"\n",
        ")\n",
        "fig_loss.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Z1d9zgBy13T0",
      "metadata": {
        "id": "Z1d9zgBy13T0"
      },
      "source": [
        "Optimizing the code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0YFHwePt7nRQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0YFHwePt7nRQ",
        "outputId": "ae9a4af6-d70d-42b9-d6a2-3cfe47300b0f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from kerastuner.tuners import RandomSearch\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "\n",
        "# Base path\n",
        "ros_dir = '/content/drive/MyDrive/cubsat/RandomOversampling'\n",
        "\n",
        "# 1. Function to load data\n",
        "def load_data(ros_dir):\n",
        "    train_X = np.load(os.path.join(ros_dir, \"train_histograms_ros.npy\"))\n",
        "    val_X = np.load(os.path.join(ros_dir, \"val_histograms_ros.npy\"))\n",
        "    train_y = np.load(os.path.join(ros_dir, \"train_labels_ros.npy\"))\n",
        "    val_y = np.load(os.path.join(ros_dir, \"val_labels_ros.npy\"))\n",
        "    return train_X, val_X, train_y, val_y\n",
        "\n",
        "# 2. Model creation function with hyperparameter search\n",
        "def build_model_hp(hp):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(\n",
        "        units=hp.Int('units_hidden1', min_value=64, max_value=256, step=32),\n",
        "        activation=hp.Choice('activation1', ['relu', 'tanh']),\n",
        "        input_shape=(train_X.shape[1],)\n",
        "    ))\n",
        "\n",
        "    # Optional Dropout\n",
        "    model.add(Dropout(hp.Float('dropout1', min_value=0.0, max_value=0.5, step=0.1)))\n",
        "\n",
        "    model.add(Dense(\n",
        "        units=hp.Int('units_hidden2', min_value=32, max_value=128, step=32),\n",
        "        activation=hp.Choice('activation2', ['relu', 'tanh'])\n",
        "    ))\n",
        "\n",
        "    model.add(Dropout(hp.Float('dropout2', min_value=0.0, max_value=0.5, step=0.1)))\n",
        "\n",
        "    model.add(Dense(n_classes, activation='softmax'))\n",
        "\n",
        "    # Choose optimizer\n",
        "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
        "    optimizer_name = hp.Choice('optimizer', ['adam', 'sgd', 'rmsprop'])\n",
        "    if optimizer_name == 'adam':\n",
        "        optimizer = Adam(learning_rate=learning_rate)\n",
        "    elif optimizer_name == 'sgd':\n",
        "        optimizer = SGD(learning_rate=learning_rate)\n",
        "    else:\n",
        "        optimizer = RMSprop(learning_rate=learning_rate)\n",
        "\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# 3. Plot with Plotly\n",
        "def plot_history_plotly(history):\n",
        "    fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Accuracy\", \"Loss\"))\n",
        "\n",
        "    fig.add_trace(go.Scatter(\n",
        "        y=history.history['accuracy'], mode='lines+markers', name='Train Accuracy'\n",
        "    ), row=1, col=1)\n",
        "\n",
        "    fig.add_trace(go.Scatter(\n",
        "        y=history.history['val_accuracy'], mode='lines+markers', name='Validation Accuracy'\n",
        "    ), row=1, col=1)\n",
        "\n",
        "    fig.add_trace(go.Scatter(\n",
        "        y=history.history['loss'], mode='lines+markers', name='Train Loss'\n",
        "    ), row=1, col=2)\n",
        "\n",
        "    fig.add_trace(go.Scatter(\n",
        "        y=history.history['val_loss'], mode='lines+markers', name='Validation Loss'\n",
        "    ), row=1, col=2)\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=\"Training History\",\n",
        "        xaxis_title=\"Epoch\",\n",
        "        xaxis2_title=\"Epoch\",\n",
        "        yaxis_title=\"Accuracy\",\n",
        "        yaxis2_title=\"Loss\",\n",
        "        template=\"plotly_white\",\n",
        "        width=1000,\n",
        "        height=400\n",
        "    )\n",
        "    fig.show()\n",
        "\n",
        "# 4. Execute pipeline\n",
        "train_X, val_X, train_y_raw, val_y_raw = load_data(ros_dir)\n",
        "n_classes = len(np.unique(train_y_raw))\n",
        "train_y = to_categorical(train_y_raw, num_classes=n_classes)\n",
        "val_y = to_categorical(val_y_raw, num_classes=n_classes)\n",
        "\n",
        "# 5. Initialize tuner\n",
        "tuner = RandomSearch(\n",
        "    build_model_hp,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=20,\n",
        "    executions_per_trial=1,\n",
        "    directory='keras_tuner_dir',\n",
        "    project_name='cubsat_tuning'\n",
        ")\n",
        "\n",
        "tuner.search_space_summary()\n",
        "\n",
        "# 6. Train with early stopping\n",
        "callbacks = [EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)]\n",
        "\n",
        "tuner.search(train_X, train_y,\n",
        "             validation_data=(val_X, val_y),\n",
        "             epochs=30,\n",
        "             batch_size=32,\n",
        "             callbacks=callbacks,\n",
        "             verbose=1)\n",
        "\n",
        "# 7. Best model\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "# 8. Train final best model\n",
        "history = best_model.fit(\n",
        "    train_X, train_y,\n",
        "    validation_data=(val_X, val_y),\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 9. Evaluate and save\n",
        "loss, acc = best_model.evaluate(val_X, val_y, verbose=0)\n",
        "print(f\"\\nFinal validation accuracy: {acc:.4f}\")\n",
        "\n",
        "# Save best model\n",
        "final_model_path = os.path.join(ros_dir, \"optimized_best_model.keras\")\n",
        "best_model.save(final_model_path)\n",
        "print(f\"Model saved at: {final_model_path}\")\n",
        "\n",
        "# 10. Visualize history\n",
        "plot_history_plotly(history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0XscAWyjBW52",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XscAWyjBW52",
        "outputId": "afa63167-22c1-4bff-c57f-5bf5b814d70d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Path to saved model\n",
        "model_path = '/content/drive/MyDrive/cubsat/RandomOversampling/optimized_best_model.keras'\n",
        "\n",
        "# Load saved model\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Reload data\n",
        "ros_dir = '/content/drive/MyDrive/cubsat/RandomOversampling'\n",
        "train_X = np.load(os.path.join(ros_dir, \"train_histograms_ros.npy\"))\n",
        "val_X = np.load(os.path.join(ros_dir, \"val_histograms_ros.npy\"))\n",
        "train_y_raw = np.load(os.path.join(ros_dir, \"train_labels_ros.npy\"))\n",
        "val_y_raw = np.load(os.path.join(ros_dir, \"val_labels_ros.npy\"))\n",
        "\n",
        "# One-hot encoding\n",
        "n_classes = len(np.unique(train_y_raw))\n",
        "train_y = to_categorical(train_y_raw, num_classes=n_classes)\n",
        "val_y = to_categorical(val_y_raw, num_classes=n_classes)\n",
        "\n",
        "# Train for 50 epochs without early stopping\n",
        "history = model.fit(\n",
        "    train_X, train_y,\n",
        "    validation_data=(val_X, val_y),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Save retrained model\n",
        "new_path = os.path.join(ros_dir, \"optimized_best_model_plus50epochs.keras\")\n",
        "model.save(new_path)\n",
        "print(f\"Retrained model saved at: {new_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HHkYV3OiCPjn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HHkYV3OiCPjn",
        "outputId": "ed6cb9a3-3a81-4ae4-ad79-1367bdda84b2"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "# === Accuracy Plot ===\n",
        "fig_acc = go.Figure()\n",
        "fig_acc.add_trace(go.Scatter(y=history.history['accuracy'], mode='lines+markers', name='Train'))\n",
        "fig_acc.add_trace(go.Scatter(y=history.history['val_accuracy'], mode='lines+markers', name='Validation'))\n",
        "fig_acc.update_layout(\n",
        "    title=\"Accuracy Evolution\",\n",
        "    xaxis_title=\"Epoch\",\n",
        "    yaxis_title=\"Accuracy\",\n",
        "    yaxis=dict(range=[0, 1]),\n",
        "    template=\"plotly_white\"\n",
        ")\n",
        "fig_acc.show()\n",
        "\n",
        "# === Loss Plot ===\n",
        "fig_loss = go.Figure()\n",
        "fig_loss.add_trace(go.Scatter(y=history.history['loss'], mode='lines+markers', name='Train'))\n",
        "fig_loss.add_trace(go.Scatter(y=history.history['val_loss'], mode='lines+markers', name='Validation'))\n",
        "fig_loss.update_layout(\n",
        "    title=\"Loss Evolution\",\n",
        "    xaxis_title=\"Epoch\",\n",
        "    yaxis_title=\"Loss\",\n",
        "    yaxis=dict(range=[0, 1]),\n",
        "    template=\"plotly_white\"\n",
        ")\n",
        "fig_loss.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vvtX1sR1zxMM",
      "metadata": {
        "id": "vvtX1sR1zxMM"
      },
      "source": [
        "TESTING THE MODEL WITH THE TEST SET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pwQfI53dzxBS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "pwQfI53dzxBS",
        "outputId": "29646882-0ef9-4ec4-f075-56f69d509b0c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import plotly.figure_factory as ff\n",
        "\n",
        "# Paths\n",
        "ros_dir = '/content/drive/MyDrive/cubsat/RandomOversampling'\n",
        "test_images_path = '/content/drive/MyDrive/cubsat/test_images.npy'\n",
        "test_labels_path = '/content/drive/MyDrive/cubsat/test_labels.npy'\n",
        "bin_edges_path = os.path.join(ros_dir, 'bin_edges_ros.npy')\n",
        "model_path = os.path.join(ros_dir, '/content/drive/MyDrive/cubsat/RandomOversampling/optimized_best_model_plus50epochs.keras')\n",
        "\n",
        "# Load data and model\n",
        "test_images = np.load(test_images_path)\n",
        "test_labels = np.load(test_labels_path)\n",
        "model = load_model(model_path)\n",
        "bin_edges = np.load(bin_edges_path, allow_pickle=True)\n",
        "\n",
        "# Histogram extraction function\n",
        "def image_to_histogram(image, bin_edges):\n",
        "    features = []\n",
        "    for channel in range(3):\n",
        "        pixels = image[:, :, channel].flatten()\n",
        "        hist, _ = np.histogram(pixels, bins=bin_edges[channel])\n",
        "        hist = hist / len(pixels)\n",
        "        features.extend(hist)\n",
        "    return np.array(features)\n",
        "\n",
        "# Convert test images to histograms\n",
        "test_histograms = np.array([image_to_histogram(img, bin_edges) for img in test_images])\n",
        "\n",
        "# Get predictions\n",
        "predictions = model.predict(test_histograms)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(test_labels, predicted_labels)\n",
        "labels = [f\"Class {i}\" for i in range(conf_matrix.shape[0])]\n",
        "\n",
        "# Plot with Plotly\n",
        "fig = ff.create_annotated_heatmap(\n",
        "    z=conf_matrix,\n",
        "    x=labels,\n",
        "    y=labels,\n",
        "    colorscale='Blues',\n",
        "    showscale=True\n",
        ")\n",
        "fig.update_layout(\n",
        "    title=\"Confusion Matrix - Test Set\",\n",
        "    xaxis_title=\"Predicted Label\",\n",
        "    yaxis_title=\"True Label\"\n",
        ")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zz3G0gKNzw59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zz3G0gKNzw59",
        "outputId": "b906cfe9-5d6f-45fa-9d80-864847b86868"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Generate report\n",
        "report = classification_report(test_labels, predicted_labels, digits=4)\n",
        "\n",
        "# Print\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WB8CnOPc1d0t",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WB8CnOPc1d0t",
        "outputId": "72249aa3-11b5-47ad-dec0-fe60a4a1f8d2"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import psutil\n",
        "import tracemalloc\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Start measuring time, memory, and CPU\n",
        "start_time = time.time()\n",
        "tracemalloc.start()\n",
        "process = psutil.Process(os.getpid())\n",
        "cpu_percent_start = psutil.cpu_percent(interval=None)\n",
        "\n",
        "# Prediction\n",
        "predictions = model.predict(test_histograms)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Metrics\n",
        "accuracy = accuracy_score(test_labels, predicted_labels)\n",
        "f1 = f1_score(test_labels, predicted_labels, average='weighted')\n",
        "\n",
        "# End measurements\n",
        "cpu_percent_end = psutil.cpu_percent(interval=None)\n",
        "_, peak_memory = tracemalloc.get_traced_memory()\n",
        "tracemalloc.stop()\n",
        "end_time = time.time()\n",
        "\n",
        "# Model code size\n",
        "model_file_path = os.path.join(ros_dir, 'dnn_histogram_model.keras')\n",
        "model_code_size = os.path.getsize(model_file_path) / 1024  # in KB\n",
        "\n",
        "# Formatted print\n",
        "print(\"Evaluation Time:        {:.4f} seconds\".format(end_time - start_time))\n",
        "print(\"Peak Memory Usage:      {:.2f} MB\".format(peak_memory / (1024 * 1024)))\n",
        "print(\"Average CPU Usage:      {:.2f}%\".format((cpu_percent_start + cpu_percent_end) / 2))\n",
        "print(\"Algorithm Code Size:    {:.2f} KB\".format(model_code_size))\n",
        "print(\"Accuracy:               {:.4f}\".format(accuracy))\n",
        "print(\"F1 Score:               {:.4f}\".format(f1))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
